# -*- coding: utf-8 -*-
"""EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fqEomXFIR6BbiLWZf6gvmo4jNeeJaCHU
"""

drive.mount("/content/drive", force_remount=True)

"""# Dataset Merging

"""

import pandas as pd

# Paths
base_path = "/content/drive/MyDrive/Thesis/"
# Geospatial data aggregation
geolocation = pd.read_csv(base_path + "Datasets/olist_geolocation_dataset.csv")

# Display the first few rows of the dataframe
print(geolocation.head())

# Get some info about the dataframe
print(geolocation.info())

# Describe the numerical features of the dataframe
print(geolocation.describe())

# Check for missing values
print(geolocation.isnull().sum())

geo_agg = geolocation.groupby('geolocation_zip_code_prefix').agg({
    'geolocation_lat': 'mean',
    'geolocation_lng': 'mean'
}).reset_index()

print(geo_agg.info())
print(geo_agg.head())

# Transactional data aggregation
orders = pd.read_csv(base_path + "Datasets/olist_orders_dataset.csv")
payments = pd.read_csv(base_path + "Datasets/olist_order_payments_dataset.csv")
items = pd.read_csv(base_path + "Datasets/olist_order_items_dataset.csv")

# Display the first few rows of the orders dataframe
print(orders.head())

# Get some info about the orders dataframe
print(orders.info())

# Describe the numerical features of the orders dataframe
print(orders.describe())

# Check for missing values in the orders dataframe
print(orders.isnull().sum())

print(payments.describe())
print(payments.info())

# prompt: show the summary of items

print(items.describe())
print(items.info())

# Merge transactional datasets
trans_data = orders.merge(payments, on='order_id', how='left')\
                   .merge(items, on='order_id', how='left')

# prompt: view the summary of trans_data

print(trans_data.info())
print(trans_data.describe())

# Behavioral data aggregation
reviews = pd.read_csv(base_path + "Datasets/olist_order_reviews_dataset.csv")
customers = pd.read_csv(base_path + "Datasets/olist_customers_dataset.csv")

print(reviews.info())
print(reviews.describe())
print(customers.info())
print(customers.describe())

reviews_orders = pd.merge(reviews, orders[['order_id', 'customer_id']], on='order_id', how='left')
behavioral_data = pd.merge(reviews_orders, customers, on='customer_id', how='left')

print(behavioral_data.head())
print(behavioral_data.isnull().sum())

# Now merge transactional + behavioral data
full_data = trans_data.merge(behavioral_data, on=['order_id', 'customer_id'], how='left')

# Finally, merge geospatial
full_data = full_data.merge(
    geo_agg, left_on='customer_zip_code_prefix', right_on='geolocation_zip_code_prefix', how='left'
)

# Check the final dataset
print(full_data.shape)
print(full_data.head())

# Check the number of unique orders and customers
print("Unique orders:", full_data['order_id'].nunique())
print("Unique customers:", full_data['customer_id'].nunique())

# Simple descriptive statistics
print(full_data.describe(include='all'))

# Check specific examples
full_data[full_data['order_id'] == 'e481f51cbdc54678b7cc49136f2d6af7']

print(full_data.isnull().sum())

date_cols = ['order_purchase_timestamp', 'review_creation_date', 'review_answer_timestamp']
for col in date_cols:
    full_data[col] = pd.to_datetime(full_data[col], errors='coerce')

from google.colab import files

# Save to temporary local disk
full_data.to_csv("full_data.csv", index=False)

# Trigger download to your local machine
files.download("full_data.csv")

# Integrated and sorted workflow
import pandas as pd
from pathlib import Path

BASE = Path('/content/drive/MyDrive/Thesis/Datasets')

# ---------- Load & light diagnostics ----------
def quick_report(df, name): print(f'{name}: {df.shape}, na%={df.isna().mean().mean():.3f}')

geo = pd.read_csv(BASE/'olist_geolocation_dataset.csv')
orders  = pd.read_csv(BASE/'olist_orders_dataset.csv', parse_dates=['order_purchase_timestamp'])
payments= pd.read_csv(BASE/'olist_order_payments_dataset.csv')
items   = pd.read_csv(BASE/'olist_order_items_dataset.csv')
reviews = pd.read_csv(BASE/'olist_order_reviews_dataset.csv', parse_dates=['review_creation_date','review_answer_timestamp'])
customers = pd.read_csv(BASE/'olist_customers_dataset.csv')

for d,n in [(geo,'geo'),(orders,'orders'),(items,'items')]: quick_report(d,n)

# ---------- Geo aggregation ----------
geo_agg = (geo
    .groupby('geolocation_zip_code_prefix', as_index=False)
    .agg(lat=('geolocation_lat','mean'), lng=('geolocation_lng','mean')))

# ---------- Transaction-level features ----------
agg_items = (items
    .groupby('order_id', as_index=False)
    .agg(n_products=('order_item_id','size'),
         n_sellers=('seller_id','nunique'),
         revenue=('price','sum'),
         freight=('freight_value','sum')))

trans = (orders
    .merge(payments, 'left', on='order_id')
    .merge(agg_items, 'left', on='order_id'))

# ---------- Behavioural & geo enrich ----------
behaviour = (reviews
    .merge(customers, 'left', on='customer_id')
    .merge(geo_agg, 'left',
           left_on='customer_zip_code_prefix',
           right_on='geolocation_zip_code_prefix'))

full_data = trans.merge(behaviour, how='left', on=['order_id','customer_id']) \
                 .drop(columns=['geolocation_zip_code_prefix'])

quick_report(full_data,'full_data')

"""# Exploratory Data Analysis"""

import matplotlib.pyplot as plt
import seaborn as sns

# Load data
full_data = pd.read_csv(base_path + "Datasets/full_data.csv")

# Check distributions
sns.histplot(full_data['payment_value'], kde=True)
plt.title('Payment Value Distribution')
plt.show()

# Correlation matrix
corr_matrix = full_data[['payment_value', 'review_score', 'geolocation_lat', 'geolocation_lng']].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# Check for missing values
print(full_data.isnull().sum())

# Explore specific columns or subsets of data
# Example 1: Examine the first 10 rows of the DataFrame
print(full_data.head(10))

# Example 3:  Examine rows where 'review_score' is less than 3
print(full_data[full_data['review_score'] < 3].head())

# Example 4: Analyze payment value distribution for different review scores
sns.boxplot(x='review_score', y='payment_value', data=full_data)
plt.title('Payment Value Distribution by Review Score')
plt.show()

# Example 5: Investigate relationship between geolocation and payment value
plt.figure(figsize=(10, 6))
sns.scatterplot(x='geolocation_lng', y='geolocation_lat', hue='payment_value', data=full_data, alpha=0.5)
plt.title('Geolocation vs. Payment Value')
plt.show()

# Example 6: Explore the relationship between payment type and review score
plt.figure(figsize=(12, 6))
sns.countplot(x='payment_type', hue='review_score', data=full_data)
plt.title('Payment Type vs. Review Score')
plt.xticks(rotation=45)
plt.show()

# Display basic information about the DataFrame
print(full_data.info())

# Display descriptive statistics
print(full_data.describe(include='all'))

"""# Data Cleaning and Preprocessing"""

# Impute numerical columns with median
numerical_cols = ['payment_sequential', 'payment_installments', 'payment_value',
                  'order_item_id', 'price', 'freight_value', 'review_score',
                  'geolocation_lat', 'geolocation_lng']

for col in numerical_cols:
    full_data[col] = full_data[col].fillna(full_data[col].median())

# Impute categorical columns with "Unknown"
categorical_cols = [
    'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date',
    'payment_type', 'product_id', 'seller_id', 'shipping_limit_date',
    'review_comment_title', 'review_comment_message',
    'review_creation_date', 'review_answer_timestamp',
    'customer_unique_id', 'customer_city', 'customer_state'
]

for col in categorical_cols:
    full_data[col].fillna('Unknown', inplace=True)

for col in categorical_cols:
    print(f"{col}:")
    print(full_data[col].value_counts(dropna=False).head(), '\n')

# Remove duplicates
full_data.drop_duplicates(inplace=True)

# Filter invalid geolocations
valid_geo = full_data[
    (full_data['geolocation_lat'].between(-90, 90)) &
    (full_data['geolocation_lng'].between(-180, 180))
]

full_data.describe(include='all')

# Check for over-imputation
for col in ['order_approved_at', 'product_id', 'seller_id', 'customer_unique_id']:
    print(f"{col} top values:\n", full_data[col].value_counts().head(), "\n")

full_data.to_csv('/content/drive/MyDrive/Thesis/Datasets/full_data_preprocessed_checkpoint.csv', index=False)

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

data = pd.read_csv('/content/drive/MyDrive/Thesis/Datasets/full_data_preprocessed_checkpoint.csv')

"""# Defining the Churn Label (90-Day Inactivity)"""

# 1. Define churn cutoff as 90 days before the last date in the data
# Convert 'order_purchase_timestamp' to datetime before finding the max
data['order_purchase_timestamp'] = pd.to_datetime(data['order_purchase_timestamp'])
max_date = data['order_purchase_timestamp'].max()
churn_cutoff_date = max_date - pd.Timedelta(days=90)

# 2. Find the last purchase date for each customer
last_purchase = data.groupby('customer_unique_id')['order_purchase_timestamp'].max()

# 3. Label churn: 1 if last purchase < cutoff (no purchase in last 90 days), else 0
churn_labels = (last_purchase < churn_cutoff_date).astype(int)

# Attach churn label to customer dataframe
customer_df = last_purchase.reset_index().rename(columns={'order_purchase_timestamp':'last_purchase_date'})
customer_df['churn'] = customer_df['customer_unique_id'].map(churn_labels)
print(customer_df[['customer_unique_id','last_purchase_date','churn']].head(5))

# Check class distribution
print(customer_df['churn'].value_counts(normalize=True))

print(max_date)

min_date = data['order_purchase_timestamp'].min()
print(min_date)

"""# Feature Engineering"""

# Temporal features
# Assuming customer_df from earlier contains each customer's last_purchase_date
reference_date = max_date  # using dataset last date as reference for recency
customer_df['recency_days'] = (reference_date - customer_df['last_purchase_date']).dt.days

# Drop duplicate order_ids to ensure each order is counted only once per customer
dedup = data[['customer_unique_id', 'order_id']].drop_duplicates()

# Compute first purchase date per customer
first_purchase = data.groupby('customer_unique_id')['order_purchase_timestamp'].min()

# Map to customer_df
customer_df['first_purchase_date'] = customer_df['customer_unique_id'].map(first_purchase)
customer_df['tenure_days'] = (reference_date - customer_df['first_purchase_date']).dt.days

# Behavioral features
# Total spending (sum of payment values for each customer)
total_spent = data.groupby('customer_unique_id')['payment_value'].sum()
customer_df['total_spent'] = customer_df['customer_unique_id'].map(total_spent)

# Average order value
customer_df['avg_order_value'] = customer_df['total_spent'] / customer_df['total_orders']

# Average review score given by the customer
avg_review_score = data.groupby('customer_unique_id')['review_score'].mean()
customer_df['avg_review_score'] = customer_df['customer_unique_id'].map(avg_review_score)

# Any bad review indicator (did customer give any review <= 2?)
bad_review_flag = data.groupby('customer_unique_id')['review_score'].apply(lambda x: 1 if any(x <= 2) else 0)
customer_df['any_bad_review'] = customer_df['customer_unique_id'].map(bad_review_flag)

# Spatial Features (Geographical Data)
# Use state as a categorical feature
# Get unique customer_state values for each customer_unique_id
customer_state_mapping = data.groupby('customer_unique_id')['customer_state'].first().to_dict()
customer_df['state'] = customer_df['customer_unique_id'].map(customer_state_mapping)

# Later we'll one-hot encode 'state' or use it in a tree-based model directly.

# Normalize latitude and longitude for use in models
# Step 1: Aggregate one set of coordinates per customer
location_df = data.groupby('customer_unique_id')[['geolocation_lat', 'geolocation_lng']].first().reset_index()

# Step 2: Merge into customer_df
customer_df = customer_df.merge(location_df, on='customer_unique_id', how='left')

# Step 3: Normalize
mean_lat, std_lat = data['geolocation_lat'].mean(), data['geolocation_lat'].std()
mean_lng, std_lng = data['geolocation_lng'].mean(), data['geolocation_lng'].std()

customer_df['lat_norm'] = (customer_df['geolocation_lat'] - mean_lat) / std_lat
customer_df['lng_norm'] = (customer_df['geolocation_lng'] - mean_lng) / std_lng

print(customer_df.head(5))

# prompt: save and download customer_df

from google.colab import files

# Assuming customer_df is already created as in your provided code
customer_df.to_csv('customer_df.csv', index=False)
files.download('customer_df.csv')

# prompt: show all the columns of data
full_data.columns

customer_df.columns

import pandas as pd
import numpy as np

customer_df = pd.read_csv('/content/drive/MyDrive/Thesis/Datasets/customer_df (1).csv',
                          parse_dates=['first_purchase_date', 'last_purchase_date'])
full_data = pd.read_csv('/content/drive/MyDrive/Thesis/Datasets/full_data_preprocessed_checkpoint.csv', parse_dates=[
    'order_purchase_timestamp',
    'order_approved_at',
    'order_delivered_customer_date',
    'order_estimated_delivery_date',
    'review_creation_date',
    'review_answer_timestamp'
])

# Order approval time (hours)
full_data['order_approved_at'] = pd.to_datetime(full_data['order_approved_at'], errors='coerce')
full_data['order_purchase_timestamp'] = pd.to_datetime(full_data['order_purchase_timestamp'], errors='coerce')
full_data['order_estimated_delivery_date'] = pd.to_datetime(full_data['order_estimated_delivery_date'], errors='coerce')
full_data['order_delivered_customer_date'] = pd.to_datetime(full_data['order_delivered_customer_date'], errors='coerce')

full_data['order_approval_time_hours'] = (full_data['order_approved_at'] -
                                          full_data['order_purchase_timestamp']).dt.total_seconds() / 3600

# Actual delivery time (days)
full_data['delivery_time_days'] = (full_data['order_delivered_customer_date']
                                   - full_data['order_purchase_timestamp']).dt.days

# On-time delivery indicator
full_data['on_time_delivery'] = (full_data['order_delivered_customer_date']
                                 <= full_data['order_estimated_delivery_date']).astype(int)

# Average metrics per customer
delivery_metrics = full_data.groupby('customer_unique_id').agg({
    'order_approval_time_hours': 'mean',
    'delivery_time_days': 'mean',
    'on_time_delivery': 'mean' # proportion of deliveries on-time
}).rename(columns={
    'order_approval_time_hours':'avg_order_approval_hours',
    'delivery_time_days':'avg_delivery_days',
    'on_time_delivery':'on_time_delivery_ratio'
})

customer_df = customer_df.merge(delivery_metrics, left_on='customer_unique_id', right_index=True)

full_data_sorted = full_data.sort_values(['customer_unique_id', 'order_purchase_timestamp'])

# Calculate inter-purchase intervals (days)
full_data_sorted['prev_purchase'] = full_data_sorted.groupby('customer_unique_id')['order_purchase_timestamp'].shift(1)
full_data_sorted['inter_purchase_days'] = (full_data_sorted['order_purchase_timestamp'] - full_data_sorted['prev_purchase']).dt.days

# Mean inter-purchase interval per customer
inter_purchase_interval = full_data_sorted.groupby('customer_unique_id')['inter_purchase_days'].mean().fillna(0)

customer_df = customer_df.merge(inter_purchase_interval.rename('avg_inter_purchase_days'), left_on='customer_unique_id', right_index=True)

customer_df['last_purchase_month'] = customer_df['last_purchase_date'].dt.month

# Seasonality (quarters as proxy for seasons)
customer_df['last_purchase_season'] = customer_df['last_purchase_date'].dt.quarter

# Preferred payment type
preferred_payment = full_data.groupby('customer_unique_id')['payment_type'] \
                             .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else 'unknown')

customer_df = customer_df.merge(preferred_payment.rename('preferred_payment_type'), left_on='customer_unique_id', right_index=True)

# Average installments
installment_usage = full_data.groupby('customer_unique_id')['payment_installments'].mean()

customer_df = customer_df.merge(installment_usage.rename('avg_payment_installments'), left_on='customer_unique_id', right_index=True)

customer_df.head(50)

freight_value = full_data.groupby('customer_unique_id')['freight_value'].mean()

customer_df = customer_df.merge(freight_value.rename('avg_freight_value'), left_on='customer_unique_id', right_index=True)

# Customer review response time (days after delivery)
full_data['review_creation_date'] = pd.to_datetime(full_data['review_creation_date'], errors='coerce')
full_data['review_answer_timestamp'] = pd.to_datetime(full_data['review_answer_timestamp'], errors='coerce')

full_data['review_response_days'] = (full_data['review_creation_date'] - full_data['order_delivered_customer_date']).dt.days

# Review answer response time (answering customer's review)
full_data['review_answer_days'] = (full_data['review_answer_timestamp'] - full_data['review_creation_date']).dt.days

# Aggregation
review_metrics = full_data.groupby('customer_unique_id').agg({
    'review_response_days': 'mean',
    'review_answer_days': 'mean'
}).rename(columns={
    'review_response_days':'avg_review_response_days',
    'review_answer_days':'avg_review_answer_days'
})

customer_df = customer_df.merge(review_metrics, left_on='customer_unique_id', right_index=True)

# Checking cases with negative values
negative_cases = full_data[full_data['review_response_days'] < 0][[
    'order_id', 'customer_unique_id',
    'order_delivered_customer_date', 'review_creation_date', 'review_response_days'
]]

print(negative_cases.head())

# Customer city
city_freq = full_data.groupby('customer_unique_id')['customer_city'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else 'unknown')
customer_df = customer_df.merge(city_freq.rename('customer_city'), left_on='customer_unique_id', right_index=True)

# Customer Zip code prefix (as categorical/frequency encoding)
zip_code_freq = full_data.groupby('customer_unique_id')['customer_zip_code_prefix'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)
customer_df = customer_df.merge(zip_code_freq.rename('customer_zip_code_prefix'), left_on='customer_unique_id', right_index=True)

full_data['review_response_days'] = (
    full_data['review_creation_date'] - full_data['order_delivered_customer_date']
).dt.days

review_metrics = full_data.groupby('customer_unique_id')['review_response_days'].mean().rename('avg_review_response_days')

customer_df = customer_df.drop(columns=['avg_review_response_days'], errors='ignore')
customer_df = customer_df.merge(
    review_metrics,
    left_on='customer_unique_id',
    right_index=True
)

customer_df.loc[customer_df['avg_review_response_days'] < 0, 'avg_review_response_days'] = 0

# Verify no negative values remain
print(customer_df['avg_review_response_days'].min())

from geopy.distance import geodesic

# Load data
seller_df = pd.read_csv('/content/drive/MyDrive/Thesis/Datasets/olist_sellers_dataset.csv')
geolocation_df = pd.read_csv('/content/drive/MyDrive/Thesis/Datasets/olist_geolocation_dataset.csv')

# Mean coordinates per seller_zip_code_prefix from geolocation data
seller_geo_coords = geolocation_df.groupby('geolocation_zip_code_prefix')[['geolocation_lat', 'geolocation_lng']].mean().reset_index()

# Rename for clarity
seller_geo_coords.rename(columns={
    'geolocation_zip_code_prefix': 'seller_zip_code_prefix',
    'geolocation_lat': 'seller_lat',
    'geolocation_lng': 'seller_lng'
}, inplace=True)

# Merge these accurate coordinates with seller_df
seller_df = seller_df.merge(seller_geo_coords, on='seller_zip_code_prefix', how='left')

full_data = full_data.merge(seller_df[['seller_id', 'seller_lat', 'seller_lng']], on='seller_id', how='left')

# Function for calculating accurate distances
def calculate_customer_seller_distance(row):
    customer_coords = (row['geolocation_lat'], row['geolocation_lng'])
    seller_coords = (row['seller_lat'], row['seller_lng'])

    # Handle missing coordinates
    if any(pd.isnull(x) for x in customer_coords + seller_coords): # Check for NaN using pd.isnull
        return np.nan

    return geodesic(customer_coords, seller_coords).km

# Apply the distance calculation
full_data['customer_seller_distance'] = full_data.apply(calculate_customer_seller_distance, axis=1)

avg_distance_customer = full_data.groupby('customer_unique_id')['customer_seller_distance'].mean().reset_index()
avg_distance_customer.rename(columns={'customer_seller_distance': 'avg_customer_seller_distance'}, inplace=True)

# Merge into customer_df
customer_df = customer_df.merge(avg_distance_customer, on='customer_unique_id', how='left')

customer_df.head(10)

# Load the data
df = pd.read_csv('/content/drive/MyDrive/Thesis/Datasets/full_data_preprocessed_checkpoint.csv')

# Step 1: Get total payment_value per order (only once per order)
order_payment = df.groupby('order_id')['payment_value'].first().reset_index()

# Step 2: Map order_id to customer_unique_id (one unique mapping per order)
order_to_customer = df[['order_id', 'customer_unique_id']].drop_duplicates(subset='order_id')

# Step 3: Merge to get customer ID for each order's payment
order_payment = order_payment.merge(order_to_customer, on='order_id', how='left')

# Step 4: Group by customer to get total spent
customer_total_spent = order_payment.groupby('customer_unique_id')['payment_value'].sum().reset_index()
customer_total_spent.rename(columns={'payment_value': 'total_spent'}, inplace=True)

# Check output
print(customer_total_spent.head(20))

# Drop the incorrect 'total_spent' column if it exists
customer_df = customer_df.drop(columns=['total_spent'], errors='ignore')

# Merge the correct total_spent
customer_df = customer_df.merge(customer_total_spent, on='customer_unique_id', how='left')

# prompt: recalculate avg_order_value using total_spent and total_orders in customer_df

# Recalculate avg_order_value
customer_df['avg_order_value'] = customer_df['total_spent'] / customer_df['total_orders']

customer_df = customer_df.drop(columns=['geolocation_lat', 'geolocation_lng', 'customer_city',
                                        'customer_zip_code_prefix', 'state', 'total_spent'], errors='ignore')

customer_df.columns

# Drop ID columns and other unneeded raw columns
customer_df_rf = customer_df.drop(columns=['customer_unique_id', 'first_purchase_date', 'last_purchase_date'])

customer_df_rf = pd.get_dummies(
    customer_df_rf,
    columns=['preferred_payment_type'],
    drop_first=True            # avoids perfect multicollinearity
)

# Check correlation matrix for high correlations
corr = customer_df_rf.corr()
print(corr['churn'].sort_values(ascending=False))

customer_df_rf.columns

full_data = pd.read_csv('/content/drive/MyDrive/Thesis/Datasets/full_data_preprocessed_checkpoint.csv')

import pandas as pd
import numpy as np

# Load the preprocessed order-level dataset (Olist e-commerce data)
orders = pd.read_csv('/content/drive/MyDrive/Thesis/Datasets/full_data_preprocessed_checkpoint.csv')
# Parse date columns
date_cols = ['order_purchase_timestamp', 'order_approved_at',
             'order_delivered_customer_date', 'order_estimated_delivery_date',
             'review_creation_date', 'review_answer_timestamp']
for col in date_cols:
    orders[col] = pd.to_datetime(orders[col], errors='coerce')

# Aggregate to order-level (one row per order)
order_group = orders.groupby('order_id').agg({
    'customer_unique_id': 'first',
    'order_purchase_timestamp': 'first',
    'order_approved_at': 'first',
    'order_delivered_customer_date': 'max',       # latest delivery in the order (if multiple items)
    'order_estimated_delivery_date': 'max',       # latest estimated delivery
    'payment_value': 'sum',                       # total payment for the order
    'freight_value': 'sum',                       # total freight for the order
    'review_score': 'first',                      # review score (one per order if available)
    'review_creation_date': 'first',
    'review_answer_timestamp': 'first'
}).reset_index(drop=True)

# Feature engineering at the order level
order_group['approval_hours'] = (order_group['order_approved_at'] - order_group['order_purchase_timestamp'])\
                                  .dt.total_seconds() / 3600.0
order_group['delivery_days'] = (order_group['order_delivered_customer_date'] - order_group['order_purchase_timestamp'])\
                                 .dt.total_seconds() / 86400.0
order_group['delivered_on_time'] = (order_group['order_delivered_customer_date'] <= order_group['order_estimated_delivery_date']).astype(int)
order_group['review_response_days'] = (order_group['review_creation_date'] - order_group['order_delivered_customer_date'])\
                                       .dt.total_seconds() / 86400.0
order_group['review_answer_days'] = (order_group['review_answer_timestamp'] - order_group['review_creation_date'])\
                                      .dt.total_seconds() / 86400.0

# Aggregate features at the customer level
customer_df = order_group.groupby('customer_unique_id').agg(
    first_purchase_date=('order_purchase_timestamp', 'min'),
    last_purchase_date=('order_purchase_timestamp', 'max'),
    total_orders=('order_purchase_timestamp', 'count'),
    total_spent=('payment_value', 'sum'),
    avg_order_value=('payment_value', 'mean'),
    avg_freight_value=('freight_value', 'mean'),
    avg_review_score=('review_score', 'mean'),
    any_bad_review=('review_score', lambda x: int((x <= 2).any())),  # 1 if any review_score ≤ 2
    avg_order_approval_hours=('approval_hours', 'mean'),
    avg_delivery_days=('delivery_days', 'mean'),
    on_time_delivery_ratio=('delivered_on_time', 'mean'),  # % of orders delivered on or before estimated date
    avg_review_response_days=('review_response_days', 'mean'),
    avg_review_answer_days=('review_answer_days', 'mean')
).reset_index()

# Compute tenure and recency in days
customer_df['tenure_days'] = (customer_df['last_purchase_date'] - customer_df['first_purchase_date']).dt.days
max_date = orders['order_purchase_timestamp'].max()  # dataset end date (last purchase timestamp in data)
customer_df['recency_days'] = (max_date - customer_df['last_purchase_date']).dt.days

# Compute average inter-purchase interval (days between consecutive orders) per customer
def avg_interpurchase(intervals):
    if len(intervals) <= 1:
        return 0.0
    # Sort purchase dates and compute day gaps
    intervals = intervals.sort_values()
    gaps = intervals.diff().dt.total_seconds() / 86400.0  # in days
    return gaps.mean()

customer_gaps = order_group.groupby('customer_unique_id')['order_purchase_timestamp'].apply(avg_interpurchase)
customer_df = customer_df.merge(customer_gaps.rename('avg_inter_purchase_days'), on='customer_unique_id', how='left')
customer_df['avg_inter_purchase_days'].fillna(0.0, inplace=True)

# Preferred payment type (most frequent) and average payment installments per customer
payment_pref = orders.groupby('customer_unique_id')['payment_type']\
                     .agg(lambda types: types.value_counts().idxmax())
installments_avg = orders.groupby('customer_unique_id')['payment_installments'].mean()
customer_df = customer_df.merge(payment_pref.rename('preferred_payment_type'), on='customer_unique_id', how='left')
customer_df = customer_df.merge(installments_avg.rename('avg_payment_installments'), on='customer_unique_id', how='left')

# Add temporal features: last purchase month and season (quarter)
customer_df['last_purchase_month'] = customer_df['last_purchase_date'].dt.month
customer_df['last_purchase_season'] = customer_df['last_purchase_month'].apply(lambda m: ((m-1)//3 + 1))

# **Churn Label (dynamic 90-day inactivity)**:
# Label churn=1 if no purchase in 90 days after last_purchase_date
customer_df['churn'] = ((max_date - customer_df['last_purchase_date']) >= pd.Timedelta(days=90)).astype(int)

# Verify new churn distribution
print(customer_df['churn'].value_counts())

# Normalize latitude and longitude
coords = orders.groupby('customer_unique_id')[['geolocation_lat', 'geolocation_lng']].first().reset_index()
lat_mean, lat_std = coords['geolocation_lat'].mean(), coords['geolocation_lat'].std()
lng_mean, lng_std = coords['geolocation_lng'].mean(), coords['geolocation_lng'].std()
coords['lat_norm'] = (coords['geolocation_lat'] - lat_mean) / lat_std
coords['lng_norm'] = (coords['geolocation_lng'] - lng_mean) / lng_std
customer_df = customer_df.merge(coords[['customer_unique_id','lat_norm','lng_norm']], on='customer_unique_id', how='left')

pd.set_option('display.max_columns', None)   # show ALL columns
pd.set_option('display.width', 0)            # auto-expand to fit terminal/Colab width
customer_df.head(20)

seller_distance = pd.read_csv('/content/drive/MyDrive/Thesis/Datasets/customer_df_rf.csv')

pd.set_option('display.max_columns', None)   # show ALL columns
pd.set_option('display.width', 0)            # auto-expand to fit terminal/Colab width
seller_distance.head(10)

# prompt: drop the tenure_days in customer_df and merge the tenure_days in seller_distance to customer_df

# Drop 'tenure_days' from customer_df if it exists
customer_df = customer_df.drop(columns=['tenure_days'], errors='ignore')

# Merge 'tenure_days' from seller_distance to customer_df
customer_df = customer_df.merge(seller_distance[['customer_unique_id', 'tenure_days']], on='customer_unique_id', how='left')

# prompt: add the avg_customer_seller_distance in seller_distance to customer_df

# Assuming 'avg_customer_seller_distance' is in seller_distance DataFrame
# and 'customer_unique_id' is the common key

customer_df = customer_df.merge(seller_distance[['customer_unique_id', 'avg_customer_seller_distance']], on='customer_unique_id', how='left')

# prompt: show a summary of customer_df

customer_df.info()
customer_df.describe()

seller_distance.info()
seller_distance.describe()

# keep the original just in case
old_customer_df = customer_df.copy()

# point `customer_df` at the seller-distance dataframe
customer_df = seller_distance.copy()      # use .copy() if you don’t want them to share memory

"""# Train/Validation/Test Split"""

# ------------------------------------------------------------------
# 1.  RANDOM – NOT TEMPORAL – TRAIN / VAL / TEST SPLIT
# ------------------------------------------------------------------
from sklearn.model_selection import train_test_split
# Optional: shuffle the frame beforehand (not strictly necessary, but makes
# sure unrelated downstream ops do not rely on the original row order)
customer_df = customer_df.sample(frac=1.0, random_state=42).reset_index(drop=True)

# (i)  hold out 20 % for final testing – stratify on churn to keep its prevalence
train_val_df, test_set = train_test_split(
    customer_df,
    test_size=0.20,
    stratify=customer_df['churn'],
    random_state=42,
)

# (ii) split the remaining 80 % into 70 % train / 10 % validation
train_set, val_set = train_test_split(
    train_val_df,
    test_size=0.125,          # 0.80 × 0.125 ≈ 0.10 of total data
    stratify=train_val_df['churn'],
    random_state=42,
)

print(f"Train set      : {len(train_set):5d}  | churn rate = {train_set['churn'].mean():.3f}")
print(f"Validation set : {len(val_set):5d}  | churn rate = {val_set['churn'].mean():.3f}")
print(f"Test set       : {len(test_set):5d}  | churn rate = {test_set['churn'].mean():.3f}")

# ------------------------------------------------------------------
# 2.  BUILD DESIGN MATRICES
# ------------------------------------------------------------------
drop_cols = ['customer_unique_id', 'first_purchase_date',
             'last_purchase_date', 'churn', 'recency_days', 'total_orders', 'tenure_days', 'avg_inter_purchase_days', 'last_purchase_month', 'last_purchase_season']

X_train, y_train = train_set.drop(columns=drop_cols), train_set['churn']
X_val,   y_val   = val_set.drop(columns=drop_cols)  , val_set['churn']
X_test,  y_test  = test_set.drop(columns=drop_cols) , test_set['churn']

# one-hot encoding of preferred_payment_type
X_train = pd.get_dummies(X_train, columns=['preferred_payment_type'])
X_val   = pd.get_dummies(X_val  , columns=['preferred_payment_type'])
X_test  = pd.get_dummies(X_test , columns=['preferred_payment_type'])

# harmonise columns across splits
X_val  = X_val.reindex (columns=X_train.columns, fill_value=0)
X_test = X_test.reindex(columns=X_train.columns, fill_value=0)
print("Features used for modeling:", list(X_train.columns))

"""# Random Forest"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV, PredefinedSplit

# Combine training and validation data for hyperparameter tuning with a predefined split
X_tune = pd.concat([X_train, X_val], ignore_index=True)
y_tune = pd.concat([y_train, y_val], ignore_index=True)
# Create an index array for PredefinedSplit: -1 for training portion, 0 for validation portion
split_index = [-1]*len(X_train) + [0]*len(X_val)
ps = PredefinedSplit(test_fold=split_index)

# Define parameter grid/distributions for Randomized Search
param_grid = {
    'n_estimators': [100, 200, 500],        # number of trees
    'max_depth': [None, 10, 20, 30],       # maximum depth of trees (None = no limit)
    'min_samples_split': [2, 5, 10],       # min samples to split an internal node
    'min_samples_leaf': [1, 2, 5],         # min samples at a leaf node
    'class_weight': [None, 'balanced']     # try without and with class balancing
}

# Initialize Random Forest classifier
rf_model = RandomForestClassifier(random_state=42)
# Randomized search across hyperparameters (e.g., 50 random combinations)
random_search = RandomizedSearchCV(
    estimator=rf_model, param_distributions=param_grid,
    n_iter=50, cv=ps, scoring='roc_auc',
    random_state=42, n_jobs=-1, verbose=1
)
random_search.fit(X_tune, y_tune)

print("Best hyperparameters:", random_search.best_params_)

# Train final model with best parameters on train+val data
best_params = random_search.best_params_
final_model = RandomForestClassifier(**best_params, random_state=42)
final_model.fit(pd.concat([X_train, X_val]), pd.concat([y_train, y_val]))

# Evaluate on the test set
y_pred = final_model.predict(X_test)
y_proba = final_model.predict_proba(X_test)[:, 1]  # probability of class 1 (churn)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report
acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)   # precision for class 1 by default
rec = recall_score(y_test, y_pred)      # recall for class 1
f1 = f1_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_proba)

print(f"Accuracy: {acc:.3f}")
print(f"Precision: {prec:.3f}")
print(f"Recall: {rec:.3f}")
print(f"F1-score: {f1:.3f}")
print(f"ROC AUC: {auc:.3f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred, digits=3))

importances = final_model.feature_importances_
feature_names = X_train.columns
feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)
print("Top 10 important features:\n", feat_imp.head(10))

"""# Convolutional Neural Network (CNN) for Spatial Data

"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# prompt: mount from google drive
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import pandas as pd
customer_df = pd.read_csv('/content/drive/MyDrive/Thesis/Datasets/customer_df_rf.csv')

# ------------------------------------------------------------------
# 1.  RANDOM – NOT TEMPORAL – TRAIN / VAL / TEST SPLIT
# ------------------------------------------------------------------
from sklearn.model_selection import train_test_split
# Optional: shuffle the frame beforehand (not strictly necessary, but makes
# sure unrelated downstream ops do not rely on the original row order)
customer_df = customer_df.sample(frac=1.0, random_state=42).reset_index(drop=True)

# (i)  hold out 20 % for final testing – stratify on churn to keep its prevalence
train_val_df, test_set = train_test_split(
    customer_df,
    test_size=0.20,
    stratify=customer_df['churn'],
    random_state=42,
)

# (ii) split the remaining 80 % into 70 % train / 10 % validation
train_set, val_set = train_test_split(
    train_val_df,
    test_size=0.125,          # 0.80 × 0.125 ≈ 0.10 of total data
    stratify=train_val_df['churn'],
    random_state=42,
)

print(f"Train set      : {len(train_set):5d}  | churn rate = {train_set['churn'].mean():.3f}")
print(f"Validation set : {len(val_set):5d}  | churn rate = {val_set['churn'].mean():.3f}")
print(f"Test set       : {len(test_set):5d}  | churn rate = {test_set['churn'].mean():.3f}")

# ------------------------------------------------------------------
# 2.  BUILD DESIGN MATRICES
# ------------------------------------------------------------------
drop_cols = ['customer_unique_id', 'first_purchase_date',
             'last_purchase_date', 'churn', 'recency_days', 'total_orders', 'tenure_days', 'avg_inter_purchase_days', 'last_purchase_month', 'last_purchase_season']

X_train, y_train = train_set.drop(columns=drop_cols), train_set['churn']
X_val,   y_val   = val_set.drop(columns=drop_cols)  , val_set['churn']
X_test,  y_test  = test_set.drop(columns=drop_cols) , test_set['churn']

# one-hot encoding of preferred_payment_type
X_train = pd.get_dummies(X_train, columns=['preferred_payment_type'])
X_val   = pd.get_dummies(X_val  , columns=['preferred_payment_type'])
X_test  = pd.get_dummies(X_test , columns=['preferred_payment_type'])

# harmonise columns across splits
X_val  = X_val.reindex (columns=X_train.columns, fill_value=0)
X_test = X_test.reindex(columns=X_train.columns, fill_value=0)
print("Features used for modeling:", list(X_train.columns))

import numpy as np

customer_images = np.load('/content/drive/MyDrive/Thesis/Datasets/customer_image_fine.npy')
print(customer_images.shape)

import numpy as np
import pandas as pd
from sklearn.preprocessing import RobustScaler

# ------------- 1. Boolean → int -----------------
bool_cols = X_train.select_dtypes(bool).columns
X_train[bool_cols] = X_train[bool_cols].astype(int)
X_val  [bool_cols] = X_val  [bool_cols].astype(int)
X_test [bool_cols] = X_test [bool_cols].astype(int)

# ------------- 2. log1p on heavy-tailed positives -------------
log_cols = [
    'avg_order_value',
    'avg_order_approval_hours',
    'avg_customer_seller_distance',
]
for col in log_cols:
    for df in (X_train, X_val, X_test):
        df[col] = np.log1p(df[col])

# ------------- 3. clip extreme outliers (99th pct) -------------
clip_cols = [
    'avg_delivery_days',
    'avg_review_answer_days',
    'avg_review_response_days',
]
for col in clip_cols:
    p99 = X_train[col].quantile(0.99)
    for df in (X_train, X_val, X_test):
        df[col] = np.clip(df[col], None, p99)

# ------------- 4. fill NaN with TRAIN mean --------------------
train_means = X_train.mean()
X_train = X_train.fillna(train_means)
X_val   = X_val.fillna(train_means)
X_test  = X_test.fillna(train_means)

# -----------------------------------------
# 1️⃣ tame the freight-value outliers
# -----------------------------------------
# OPTION A – log-transform  (recommended)
for df in (X_train, X_val, X_test):
    df['avg_freight_value'] = np.log1p(df['avg_freight_value'])

# ----- OR -----
# OPTION B – clip at 99th-percentile
# p99 = X_train['avg_freight_value'].quantile(0.99)
# for df in (X_train, X_val, X_test):
#     df['avg_freight_value'] = np.clip(df['avg_freight_value'], None, p99)

# -----------------------------------------
# 2️⃣ re-scale
# -----------------------------------------
from sklearn.preprocessing import RobustScaler
scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled   = scaler.transform(X_val)
X_test_scaled  = scaler.transform(X_test)

# quick sanity check
print(np.min(X_train_scaled), np.max(X_train_scaled))

# indices-based masks
train_mask = customer_df.index.isin(train_set.index)
val_mask = customer_df.index.isin(val_set.index)
test_mask = customer_df.index.isin(test_set.index)

X_train_img = customer_images[train_mask]
X_val_img = customer_images[val_mask]
X_test_img = customer_images[test_mask]

# Labels
y_train = train_set['churn'].values
y_val = val_set['churn'].values
y_test = test_set['churn'].values

print(X_train_scaled.shape, X_val_scaled.shape, X_test_scaled.shape)
print(X_train_img.shape, X_val_img.shape, X_test_img.shape)

print(X_train_img.min(), X_train_img.max())

# 2. DEFINE MODEL ARCHITECTURE (Hybrid CNN + MLP)
# Determine input shapes
num_features = X_train_scaled.shape[1]
image_shape = X_train_img.shape[1:]  # shape of one image (e.g., (height, width, channels))

# Numeric branch (MLP)
inp_num = keras.Input(shape=(num_features,), name="numeric_input")
x_num = layers.Dense(64, activation='relu')(inp_num)
x_num = layers.Dropout(0.3)(x_num)  # dropout for regularization
x_num = layers.Dense(32, activation='relu')(x_num)

# Image branch (CNN)
inp_img = keras.Input(shape=image_shape, name="image_input")
# Example CNN layers (adjust filter sizes and architecture as appropriate for your images)
x_img = layers.Conv2D(32, kernel_size=(3,3), activation='relu')(inp_img)
x_img = layers.MaxPooling2D(pool_size=(2,2))(x_img)
x_img = layers.Conv2D(64, kernel_size=(3,3), activation='relu')(x_img)
x_img = layers.MaxPooling2D(pool_size=(2,2))(x_img)
x_img = layers.Flatten()(x_img)  # flatten to vector
x_img = layers.Dense(64, activation='relu')(x_img)

# Combine branches
combined = layers.concatenate([x_num, x_img])
combined = layers.Dense(32, activation='relu')(combined)
combined = layers.Dropout(0.2)(combined)
output = layers.Dense(1, activation='sigmoid')(combined)

model = keras.Model(inputs=[inp_num, inp_img], outputs=output)
# Print model summary (optional)
model.summary()

# 3. COMPILE MODEL WITH OPTIONAL CLASS IMBALANCE HANDLING
# Option A: Use Focal Loss (and optionally class weights)
def focal_loss(alpha=1.0, gamma=2.0):
    # Focal loss for binary classification
    def loss(y_true, y_pred):
        # Clip predictions to avoid log(0)
        epsilon = tf.keras.backend.epsilon()
        y_pred = tf.keras.backend.clip(y_pred, epsilon, 1.0 - epsilon)
        # Compute cross-entropy
        ce_loss = - (y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred))
        # Compute focal weight
        p_t = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)
        focal_factor = tf.pow(1 - p_t, gamma)
        return tf.reduce_mean(alpha * focal_factor * ce_loss)
    return loss

USE_FOCAL_LOSS = True  # toggle this to True to use focal loss
if USE_FOCAL_LOSS:
    loss_fn = focal_loss(alpha=1.0, gamma=2.0)
else:
    loss_fn = 'binary_crossentropy'

# We can also use class weights to handle imbalance (if not oversampling):
pos = np.sum(y_train == 1)
neg = np.sum(y_train == 0)
class_weight = {0: (pos/neg),   # ≈ 9.0  → up-weight non-churn
                1: 1.0}


model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])

# 4. TRAIN MODEL (with optional oversampling or class weights)
USE_OVERSAMPLING = False  # toggle to True to perform oversampling of minority class
if USE_OVERSAMPLING:
    # Oversample minority class in training set
    churn_indices = np.where(y_train == 1)[0]
    nonchurn_indices = np.where(y_train == 0)[0]
    # Determine how many to oversample to balance classes:
    n_churn = len(churn_indices)
    n_nonchurn = len(nonchurn_indices)
    if n_churn == 0 or n_nonchurn == 0:
        print("Error: one of the classes has zero samples in training!")
    else:
        if n_churn < n_nonchurn:
            # oversample churn
            oversample_count = n_nonchurn - n_churn
            # random choice of churn indices to duplicate
            dup_indices = np.random.choice(churn_indices, size=oversample_count, replace=True)
            X_train_os_num = np.vstack([X_train_scaled, X_train_scaled[dup_indices]])
            X_train_os_img = np.vstack([X_train_img, X_train_img[dup_indices]])
            y_train_os = np.concatenate([y_train, y_train[dup_indices]])
        else:
            # oversample non-churn (in case churn > nonchurn, less likely)
            oversample_count = n_churn - n_nonchurn
            dup_indices = np.random.choice(nonchurn_indices, size=oversample_count, replace=True)
            X_train_os_num = np.vstack([X_train_scaled, X_train_scaled[dup_indices]])
            X_train_os_img = np.vstack([X_train_img, X_train_img[dup_indices]])
            y_train_os = np.concatenate([y_train, y_train[dup_indices]])
        # Shuffle the oversampled training data
        perm = np.random.permutation(len(y_train_os))
        X_train_os_num = X_train_os_num[perm]
        X_train_os_img = X_train_os_img[perm]
        y_train_os = y_train_os[perm]
    train_num_data = X_train_os_num
    train_img_data = X_train_os_img
    train_labels = y_train_os
    use_class_weight = False  # if oversampling, we typically don't use class weights
else:
    train_num_data = X_train_scaled
    train_img_data = X_train_img
    train_labels = y_train
    use_class_weight = True  # if not oversampling, consider class_weight

# Train the model, using validation data for monitoring
epochs = 20
batch_size = 32
callbacks = [keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)]
if use_class_weight: # and not USE_FOCAL_LOSS:
    history = model.fit(
        {"numeric_input": train_num_data, "image_input": train_img_data}, train_labels,
        validation_data= ({"numeric_input": X_val_scaled, "image_input": X_val_img}, y_val),
        epochs=epochs, batch_size=batch_size, class_weight=class_weight, callbacks=callbacks, verbose=2
    )
else:
    history = model.fit(
        {"numeric_input": train_num_data, "image_input": train_img_data}, train_labels,
        validation_data= ({"numeric_input": X_val_scaled, "image_input": X_val_img}, y_val),
        epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=2
    )

## focal loss and class weight
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import (roc_curve, roc_auc_score,
                             precision_recall_curve, average_precision_score,
                             classification_report, confusion_matrix,
                             precision_score, recall_score, f1_score)

# ───────────────────────────────────────────────────────────────
# 1) PROBABILITIES & AUC METRICS
# ───────────────────────────────────────────────────────────────
y_val_pred_prob  = model.predict(
        {"numeric_input": X_val_scaled,  "image_input": X_val_img},
        verbose=0).ravel()
y_test_pred_prob = model.predict(
        {"numeric_input": X_test_scaled, "image_input": X_test_img},
        verbose=0).ravel()

val_roc_auc  = roc_auc_score(y_val,  y_val_pred_prob)
val_pr_auc   = average_precision_score(y_val,  y_val_pred_prob)
test_roc_auc = roc_auc_score(y_test, y_test_pred_prob)
test_pr_auc  = average_precision_score(y_test, y_test_pred_prob)

print(f"Validation  ROC-AUC : {val_roc_auc:.3f} | PR-AUC : {val_pr_auc:.3f}")
print(f"Test        ROC-AUC : {test_roc_auc:.3f} | PR-AUC : {test_pr_auc:.3f}")

# ───────────────────────────────────────────────────────────────
# 2) ROC  &  PR CURVES  (test)
# ───────────────────────────────────────────────────────────────
# ROC
fpr, tpr, _ = roc_curve(y_test, y_test_pred_prob)
plt.figure(figsize=(5.5,5))
plt.plot(fpr, tpr, label=f"AUC = {test_roc_auc:.2f}")
plt.plot([0,1],[0,1],'k--', lw=0.8)
plt.xlabel("False-Positive Rate");  plt.ylabel("True-Positive Rate (recall)")
plt.title("ROC curve – test");      plt.legend();  plt.grid(alpha=.2);  plt.show()

# PR
prec, rec, _ = precision_recall_curve(y_test, y_test_pred_prob)
baseline = np.mean(y_test)
plt.figure(figsize=(5.5,5))
plt.plot(rec, prec, label=f"AUC = {test_pr_auc:.2f}")
plt.hlines(baseline, 0, 1, colors='gray', linestyles='--',
           label=f"Baseline (precision={baseline:.2f})")
plt.xlabel("Recall"); plt.ylabel("Precision"); plt.title("PR curve – test")
plt.legend(); plt.grid(alpha=.2); plt.show()

# ───────────────────────────────────────────────────────────────
# 3) THRESHOLD SWEEP  (validation set)
#    we scan 0.05 … 0.95 in 5-point steps
# ───────────────────────────────────────────────────────────────
scan_thr = np.linspace(0.05, 0.95, 19)
rows = []
for t in scan_thr:
    y_hat = (y_val_pred_prob >= t).astype(int)
    rec0  = recall_score   (y_val, y_hat, pos_label=0)
    prec0 = precision_score(y_val, y_hat, pos_label=0, zero_division=0)
    rec1  = recall_score   (y_val, y_hat, pos_label=1)
    prec1 = precision_score(y_val, y_hat, pos_label=1, zero_division=0)

    # balanced-F1  (average of class-specific F1)
    balF1 = (f1_score(y_val, y_hat, pos_label=0) +
             f1_score(y_val, y_hat, pos_label=1)) / 2

    rows.append([t, rec0, prec0, rec1, prec1, balF1])

tbl = pd.DataFrame(rows, columns=
        ["thr","recall_0","prec_0","recall_1","prec_1","balF1"])

print("\nThreshold sweep on validation set (sorted by balanced-F1):")
display(tbl.sort_values("balF1", ascending=False).head(20))

# choose threshold rule  👉  here: max balanced-F1
best_thr = tbl.loc[tbl["balF1"].idxmax(), "thr"]
print(f"\nChosen threshold (max balanced-F1) = {best_thr:.2f}")

# ───────────────────────────────────────────────────────────────
# 4) FINAL TEST-SET EVALUATION AT CHOSEN THRESHOLD
# ───────────────────────────────────────────────────────────────
y_test_pred = (y_test_pred_prob >= best_thr).astype(int)

cm = confusion_matrix(y_test, y_test_pred)
print("\nConfusion matrix (test):")
print(cm)

plt.figure(figsize=(4,3))
sns.heatmap(cm, annot=True, fmt='d', cmap="Blues",
            xticklabels=["Pred•No-Churn","Pred•Churn"],
            yticklabels=["Act•No-Churn","Act•Churn"])
plt.title(f"Threshold = {best_thr:.2f}"); plt.ylabel(""); plt.xlabel("")
plt.show()

print("\nClassification report (test):")
print(classification_report(y_test, y_test_pred,
                            target_names=["No-Churn","Churn"],
                            digits=3))

"""# Long Short-Term Memory (LSTM) for Temporal Data

"""

# prompt: mount from google drive
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

!pip install -q keras-tuner

import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix, classification_report, precision_recall_curve
import matplotlib.pyplot as plt
import seaborn as sns
import keras_tuner as kt

"""
End‑to‑end sequence construction pipeline
-----------------------------------------
* Loads the pre‑processed Olist checkout file.
* Aggregates to order‑level granularity (one row per order).
* Engineers:
    - item_count
    - one‑hot payment‑type flags
    - max_installments
    - days‑since‑previous‑order (inter‑purchase gap)
* Builds customer‑level sequences and pads to equal length.
"""

import pandas as pd
import numpy as np

# ------------------------------------------------------------------
# 0. PARAMETERS
# ------------------------------------------------------------------
RAW_PATH   = '/content/drive/MyDrive/Thesis/Datasets/full_data_preprocessed_checkpoint.csv'
DATE_COL   = 'order_purchase_timestamp'
PAY_TYPES  = ['credit_card', 'boleto', 'voucher', 'debit_card']   # adapt if needed
REVIEW_IMPUTE_VALUE = 0        # e.g. 0 → "no review"

# ------------------------------------------------------------------
# 1. LOAD ITEM‑LEVEL DATA (minimal columns)
# ------------------------------------------------------------------
use_cols = [
    'customer_unique_id', 'order_id', DATE_COL,
    'price', 'freight_value',
    'payment_type', 'payment_installments',
    'review_score'
]
orders = pd.read_csv(
    RAW_PATH,
    usecols     = use_cols,
    parse_dates = [DATE_COL]
)

# ------------------------------------------------------------------
# 2. AGGREGATE TO ORDER‑LEVEL
# ------------------------------------------------------------------
order_agg = (
    orders
      .groupby('order_id', as_index=False)        # keep order_id as column
      .agg(customer_unique_id      = ('customer_unique_id', 'first'),
           order_purchase_timestamp= (DATE_COL, 'first'),
           price                   = ('price', 'sum'),
           freight_value           = ('freight_value', 'sum'),
           payment_type            = ('payment_type', list),
           payment_installments    = ('payment_installments', list),
           review_score            = ('review_score', 'first'),
           item_count              = ('payment_type', 'size'))     # #rows == #items
)

# ------------------------------------------------------------------
# 3. FEATURE ENGINEERING
# ------------------------------------------------------------------
# 3.1 One‑hot payment‑type indicators
for p in PAY_TYPES:
    order_agg[f'pay_{p}'] = order_agg['payment_type'].apply(lambda lst: int(p in lst))

# 3.2 Maximum installments within the order
order_agg['max_installments'] = order_agg['payment_installments'].apply(
    lambda lst: max(lst) if lst else 1
)

# 3.3 Impute missing review scores
order_agg['review_score'] = order_agg['review_score'].fillna(REVIEW_IMPUTE_VALUE)

# 3.4 Chronological sort inside each customer
order_agg.sort_values(['customer_unique_id', 'order_purchase_timestamp'],
                      inplace=True)

# ------------------------------------------------------------------
# 4. CONSTRUCT CUSTOMER‑LEVEL SEQUENCES
# ------------------------------------------------------------------
sequences, customer_ids = [], []
for cust_id, grp in order_agg.groupby('customer_unique_id'):
    grp = grp.sort_values('order_purchase_timestamp')

    seq = []
    for _, row in grp.iterrows():
        features = [
            row['price'],
            row['freight_value'],
            row['item_count'],
            row['review_score'],
            row['max_installments']
        ] + [row[f'pay_{p}'] for p in PAY_TYPES]
        seq.append(features)

    customer_ids.append(cust_id)
    sequences.append(seq)


# ------------------------------------------------------------------
# 5. ZERO‑PAD TO A FIXED LENGTH TENSOR
# ------------------------------------------------------------------
max_seq_len  = max(len(seq) for seq in sequences)
num_features = 5 + len(PAY_TYPES)

X_seq_all = np.zeros((len(sequences), max_seq_len, num_features))

for i, seq in enumerate(sequences):
    seq_arr = np.asarray(seq, dtype=float)
    X_seq_all[i, :len(seq_arr), :] = seq_arr   # left‑aligned, zero‑padded

# ------------------------------------------------------------------
# 6. QUICK VALIDATION
# ------------------------------------------------------------------
print("Tensor shape  :", X_seq_all.shape)        # (customers, timesteps, features)
print("Feature names :", [
    'price', 'freight', 'item_count',
    'review_score', 'max_installments'
] + [f'pay_{p}' for p in PAY_TYPES])
print("Max sequence length :", max_seq_len)

# Load static customer features to get churn labels (and possibly to cross-verify or use directly)
customer_static = pd.read_csv('/content/drive/MyDrive/Thesis/Datasets/customer_df_rf.csv', usecols=['customer_unique_id', 'churn', 'last_purchase_date', 'recency_days'])
customer_static.set_index('customer_unique_id', inplace=True)

# Compute churn labels for our sequence data
y_all = []
for cust_id in customer_ids:
    if cust_id not in customer_static.index:
        # If a customer ID from sequence is not in static (unlikely if data sources match), skip or assume churn
        label = 1
    else:
        label = customer_static.loc[cust_id, 'churn']
    y_all.append(label)
y_all = np.array(y_all)

# (Alternatively, compute label by checking last purchase gap manually)
# Example manual approach (if not using customer_static):
# last_date = order_agg[order_agg['customer_unique_id'] == cust_id]['order_purchase_timestamp'].max()
# recency = (ref_date - last_date).days
# label = 1 if recency > 90 else 0

# Prepare static feature matrix
static_features = ['avg_order_value', 'avg_review_score',
                   'any_bad_review', 'on_time_delivery_ratio', 'lat_norm', 'lng_norm', 'avg_order_approval_hours',
                   'avg_delivery_days', 'on_time_delivery_ratio', 'avg_review_answer_days',
                   'preferred_payment_type', 'avg_payment_installments', 'avg_freight_value', 'avg_review_response_days',
                   'avg_customer_seller_distance']
# Note: We include preferred_payment_type which is categorical
# We will one-hot encode preferred_payment_type
static_df = pd.read_csv('/content/drive/MyDrive/Thesis/Datasets/customer_df_rf.csv', usecols=['customer_unique_id'] + static_features)
static_df.set_index('customer_unique_id', inplace=True)

# One-hot encode 'preferred_payment_type'
static_df = pd.get_dummies(static_df, columns=['preferred_payment_type'], prefix='pref_pay')

# Align static features with our sequence list order
X_static_all = []
for cust_id in customer_ids:
    # If customer not in static_df (should not happen if data is consistent), handle gracefully
    if cust_id in static_df.index:
        X_static_all.append(static_df.loc[cust_id].values.astype(float))
    else:
        X_static_all.append(np.zeros(static_df.shape[1]))  # placeholder if missing
X_static_all = np.stack(X_static_all)

# Optionally, scale static features (e.g., StandardScaler or MinMaxScaler)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_static_all = scaler.fit_transform(X_static_all)

# Determine class imbalance
import numpy as np
n_churn = np.sum(y_all == 1)
n_not_churn = np.sum(y_all == 0)
print(f"Churners: {n_churn}, Non-churners: {n_not_churn}")

# Compute class weights as inverse of frequency
total = len(y_all)
weight_for_churn = total / (2 * n_churn)
weight_for_not_churn = total / (2 * n_not_churn)
class_weight = {0: weight_for_not_churn, 1: weight_for_churn}
print("Class weight:", class_weight)

from tensorflow.keras.layers import Input, LSTM, Bidirectional, Dense, Dropout, Masking, concatenate
from tensorflow.keras.models import Model

def build_lstm_model(seq_input_shape, static_input_shape=None,
                     num_lstm_layers=1, units_per_layer=[64], bidirectional=False, dropout_rate=0.0):
    """
    Build an LSTM model with given architecture parameters.
    seq_input_shape: tuple for sequence input (max_seq_len, num_features)
    static_input_shape: tuple for static input features (num_static_features,), or None if not used
    num_lstm_layers: how many LSTM layers to stack
    units_per_layer: list of LSTM units for each layer (length should == num_lstm_layers)
    bidirectional: whether to use Bidirectional LSTM for each layer
    dropout_rate: dropout to apply after each LSTM layer (except maybe last) and on final dense layer
    """
    # Define sequence input
    seq_input = Input(shape=seq_input_shape, name="sequence_input")
    x = Masking(mask_value=0.0, name="masking_layer")(seq_input)  # ignore padded zeros

    # Add LSTM layers
    for i in range(num_lstm_layers):
        units = units_per_layer[i]
        if bidirectional:
            x = Bidirectional(LSTM(units, return_sequences=(i < num_lstm_layers - 1),
                                    name=f"LSTM_layer_{i+1}",))(x)
        else:
            x = LSTM(units, return_sequences=(i < num_lstm_layers - 1), name=f"LSTM_layer_{i+1}")(x)
        # Apply dropout between layers if specified
        if dropout_rate > 0.0:
            x = Dropout(dropout_rate, name=f"dropout_layer_{i+1}")(x)

    # If static features are provided, define static input and concatenate
    if static_input_shape:
        static_input = Input(shape=static_input_shape, name="static_input")
        # (Optionally, you could add a Dense layer here to transform static input if needed)
        combined = concatenate([x, static_input], name="concatenate_layer")
    else:
        static_input = None
        combined = x

    # Add a fully connected layer after concatenation (if desired)
    combined = Dense(64, activation='relu', name="dense_after_concat")(combined)
    if dropout_rate > 0.0:
        combined = Dropout(dropout_rate, name="dropout_after_concat")(combined)
    # Output layer
    output = Dense(1, activation='sigmoid', name="output_layer")(combined)

    # Build the model
    if static_input_shape:
        model = Model(inputs=[seq_input, static_input], outputs=output)
    else:
        model = Model(inputs=seq_input, outputs=output)
    return model

# Example: a 2-layer Bidirectional LSTM model
example_model = build_lstm_model(seq_input_shape=(max_seq_len, num_features),
                                 static_input_shape=(X_static_all.shape[1],),
                                 num_lstm_layers=2, units_per_layer=[64, 32], bidirectional=True, dropout_rate=0.2)
example_model.summary()

from sklearn.preprocessing import StandardScaler

seq_scaler = StandardScaler()
num_seq_features = 5        # first five columns are numeric

# flatten -> fit scaler -> reshape
flat = X_seq_all[:, :, :num_seq_features].reshape(-1, num_seq_features)
flat = seq_scaler.fit_transform(flat)
X_seq_all[:, :, :num_seq_features] = flat.reshape(X_seq_all.shape[0],
                                                 X_seq_all.shape[1],
                                                 num_seq_features)

from sklearn.model_selection import train_test_split

# First, combine X_seq_all and X_static_all into appropriate structures
# (We already have them as numpy arrays.)

X_seq_train, X_seq_temp, X_static_train, X_static_temp, y_train, y_temp = train_test_split(
    X_seq_all, X_static_all, y_all, test_size=0.30, stratify=y_all, random_state=42)
# Now split the temp (30%) into val and test equally (15% each of total, i.e., 50/50 split of temp)
X_seq_val, X_seq_test, X_static_val, X_static_test, y_val, y_test = train_test_split(
    X_seq_temp, X_static_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=42)
print("Shapes:", X_seq_train.shape, X_seq_val.shape, X_seq_test.shape)
print("Churn rate in train: ", y_train.mean(),
      "val:", y_val.mean(), "test:", y_test.mean())

import keras_tuner as kt
from tensorflow.keras import backend as K

def focal_loss(gamma=2., alpha=0.25):
    def _fl(y_true, y_pred):
        y_true = K.cast(y_true, K.floatx())
        # ❶ clip to prevent log(0)
        y_pred = K.clip(y_pred, K.epsilon(), 1. - K.epsilon())

        bce = K.binary_crossentropy(y_true, y_pred)
        pt  = K.exp(-bce)
        loss = alpha * K.pow(1. - pt, gamma) * bce
        # ❷ replace any remaining NaN/Inf with finite numbers
        loss = tf.where(tf.math.is_finite(loss), loss, tf.zeros_like(loss))
        return K.mean(loss)
    return _fl

# Hyperparameter tuning setup corrected
def build_model(hp):
    num_layers = hp.Int('num_lstm_layers', 1, 2, step=1)
    units = [hp.Choice(f'units_layer{i+1}', [32, 64]) for i in range(num_layers)]
    bidir = hp.Boolean('bidirectional')
    dropout_rate = hp.Float('dropout_rate', 0.0, 0.3, step=0.1)
    learning_rate = hp.Choice('learning_rate', [3e-4, 1e-4, 5e-5, 1e-5])

    model = build_lstm_model(
        seq_input_shape=(max_seq_len, num_features),
        static_input_shape=(X_static_all.shape[1],),
        num_lstm_layers=num_layers,
        units_per_layer=units,
        bidirectional=bidir,
        dropout_rate=dropout_rate
    )

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
        loss=focal_loss(gamma=2., alpha=0.25),  # ✅ Use focal loss
        metrics=[
            tf.keras.metrics.AUC(name='AUC'),
            'accuracy',
            tf.keras.metrics.Precision(name='precision'),
            tf.keras.metrics.Recall(name='recall')
        ]
    )

    return model

tuner = kt.Hyperband(
    build_model,
    objective='val_AUC',
    max_epochs=10,
    factor=3,
    directory='kt_tuning',
    project_name='olist_churn_lstm'
)

# Run tuner WITHOUT class_weight
tuner.search(
    [X_seq_train, X_static_train], y_train,
    validation_data=([X_seq_val, X_static_val], y_val),
    epochs=20, batch_size=128,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(
            patience=3, restore_best_weights=True,
            monitor='val_AUC', mode='max'
        )
    ]
)

# Best hyperparameters from tuning
best_model = tuner.get_best_models(1)[0]

# Predict and evaluate directly from best_model
y_proba = best_model.predict([X_seq_test, X_static_test]).ravel()
roc_auc = roc_auc_score(y_test, y_proba)
pr_auc = average_precision_score(y_test, y_proba)

# Find optimal threshold on validation (for example, maximize F1)
precision, recall, thresholds = precision_recall_curve(y_val, best_model.predict([X_seq_val, X_static_val]).ravel())
f1 = 2*(precision*recall)/(precision+recall+1e-8)
best_thresh = thresholds[np.argmax(f1)]
print(f"Optimal threshold: {best_thresh:.3f}")
best_thresh = 0.6
# Apply threshold to test set
y_pred = (y_proba >= best_thresh).astype(int)
cm = confusion_matrix(y_test, y_pred)
print(classification_report(y_test, y_pred))

# Confusion matrix and classification report at this threshold
cm = confusion_matrix(y_test, y_pred)

print("Confusion Matrix:\n", cm)
print("Classification Report:\n", classification_report(y_test, y_pred, digits=4))

import matplotlib.pyplot as plt
import seaborn as sns

# 1. Training history plot
plt.figure(figsize=(6,4))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.plot(history.history.get('AUC', []), label='Train AUC')
plt.plot(history.history.get('val_AUC', []), label='Val AUC')
plt.title('Training History')
plt.xlabel('Epoch')
plt.legend()
plt.show()

# 2. ROC curve plot
from sklearn.metrics import roc_curve, precision_recall_curve, auc
fpr, tpr, _ = roc_curve(y_test, y_proba)
roc_auc_val = auc(fpr, tpr)
plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0,1], [0,1], 'k--', label='Random guess')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate (Recall)')
plt.title('ROC Curve - Test Set')
plt.legend(loc='lower right')
plt.show()

# 3. Precision-Recall curve plot
precision, recall, _ = precision_recall_curve(y_test, y_proba)
pr_auc_val = auc(recall, precision)
plt.figure(figsize=(6,5))
plt.plot(recall, precision, label=f'PR curve (AUC = {pr_auc:.2f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve - Test Set')
plt.legend(loc='upper right')
plt.show()

# 4. Confusion matrix heatmap
plt.figure(figsize=(4,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Predicted Negative','Predicted Positive'],
            yticklabels=['Actual Negative','Actual Positive'])
plt.title(f'Confusion Matrix @ threshold={best_thresh:.2f}')
plt.show()

"""# CNN + LSTM

"""

# prompt: mount from google drive
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

!pip install -q keras-tuner

import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix, classification_report, precision_recall_curve
import matplotlib.pyplot as plt
import seaborn as sns
import keras_tuner as kt
import numpy as np

# ─── load 32×32×1 images ─────────────────────────────────────────
customer_images = np.load('/content/drive/MyDrive/Thesis/Datasets/customer_image_fine.npy')

# sanity
print("Images:", customer_images.shape, customer_images.min(), customer_images.max())

"""
End‑to‑end sequence construction pipeline
-----------------------------------------
* Loads the pre‑processed Olist checkout file.
* Aggregates to order‑level granularity (one row per order).
* Engineers:
    - item_count
    - one‑hot payment‑type flags
    - max_installments
    - days‑since‑previous‑order (inter‑purchase gap)
* Builds customer‑level sequences and pads to equal length.
"""

import pandas as pd
import numpy as np

# ------------------------------------------------------------------
# 0. PARAMETERS
# ------------------------------------------------------------------
RAW_PATH   = '/content/drive/MyDrive/Thesis/Datasets/full_data_preprocessed_checkpoint.csv'
DATE_COL   = 'order_purchase_timestamp'
PAY_TYPES  = ['credit_card', 'boleto', 'voucher', 'debit_card']   # adapt if needed
REVIEW_IMPUTE_VALUE = 0        # e.g. 0 → "no review"

# ------------------------------------------------------------------
# 1. LOAD ITEM‑LEVEL DATA (minimal columns)
# ------------------------------------------------------------------
use_cols = [
    'customer_unique_id', 'order_id', DATE_COL,
    'price', 'freight_value',
    'payment_type', 'payment_installments',
    'review_score'
]
orders = pd.read_csv(
    RAW_PATH,
    usecols     = use_cols,
    parse_dates = [DATE_COL]
)

# ------------------------------------------------------------------
# 2. AGGREGATE TO ORDER‑LEVEL
# ------------------------------------------------------------------
order_agg = (
    orders
      .groupby('order_id', as_index=False)        # keep order_id as column
      .agg(customer_unique_id      = ('customer_unique_id', 'first'),
           order_purchase_timestamp= (DATE_COL, 'first'),
           price                   = ('price', 'sum'),
           freight_value           = ('freight_value', 'sum'),
           payment_type            = ('payment_type', list),
           payment_installments    = ('payment_installments', list),
           review_score            = ('review_score', 'first'),
           item_count              = ('payment_type', 'size'))     # #rows == #items
)

# ------------------------------------------------------------------
# 3. FEATURE ENGINEERING
# ------------------------------------------------------------------
# 3.1 One‑hot payment‑type indicators
for p in PAY_TYPES:
    order_agg[f'pay_{p}'] = order_agg['payment_type'].apply(lambda lst: int(p in lst))

# 3.2 Maximum installments within the order
order_agg['max_installments'] = order_agg['payment_installments'].apply(
    lambda lst: max(lst) if lst else 1
)

# 3.3 Impute missing review scores
order_agg['review_score'] = order_agg['review_score'].fillna(REVIEW_IMPUTE_VALUE)

# 3.4 Chronological sort inside each customer
order_agg.sort_values(['customer_unique_id', 'order_purchase_timestamp'],
                      inplace=True)

# ------------------------------------------------------------------
# 4. CONSTRUCT CUSTOMER‑LEVEL SEQUENCES
# ------------------------------------------------------------------
sequences, customer_ids = [], []
for cust_id, grp in order_agg.groupby('customer_unique_id'):
    grp = grp.sort_values('order_purchase_timestamp')

    seq = []
    for _, row in grp.iterrows():
        features = [
            row['price'],
            row['freight_value'],
            row['item_count'],
            row['review_score'],
            row['max_installments']
        ] + [row[f'pay_{p}'] for p in PAY_TYPES]
        seq.append(features)

    customer_ids.append(cust_id)
    sequences.append(seq)


# ------------------------------------------------------------------
# 5. ZERO‑PAD TO A FIXED LENGTH TENSOR
# ------------------------------------------------------------------
max_seq_len  = max(len(seq) for seq in sequences)
num_features = 5 + len(PAY_TYPES)

X_seq_all = np.zeros((len(sequences), max_seq_len, num_features))

for i, seq in enumerate(sequences):
    seq_arr = np.asarray(seq, dtype=float)
    X_seq_all[i, :len(seq_arr), :] = seq_arr   # left‑aligned, zero‑padded

# ------------------------------------------------------------------
# 6. QUICK VALIDATION
# ------------------------------------------------------------------
print("Tensor shape  :", X_seq_all.shape)        # (customers, timesteps, features)
print("Feature names :", [
    'price', 'freight', 'item_count',
    'review_score', 'max_installments'
] + [f'pay_{p}' for p in PAY_TYPES])
print("Max sequence length :", max_seq_len)

# Load static customer features to get churn labels (and possibly to cross-verify or use directly)
customer_static = pd.read_csv('/content/drive/MyDrive/Thesis/Datasets/customer_df_rf.csv', usecols=['customer_unique_id', 'churn', 'last_purchase_date', 'recency_days'])
customer_static.set_index('customer_unique_id', inplace=True)

# Compute churn labels for our sequence data
y_all = []
for cust_id in customer_ids:
    if cust_id not in customer_static.index:
        # If a customer ID from sequence is not in static (unlikely if data sources match), skip or assume churn
        label = 1
    else:
        label = customer_static.loc[cust_id, 'churn']
    y_all.append(label)
y_all = np.array(y_all)

# (Alternatively, compute label by checking last purchase gap manually)
# Example manual approach (if not using customer_static):
# last_date = order_agg[order_agg['customer_unique_id'] == cust_id]['order_purchase_timestamp'].max()
# recency = (ref_date - last_date).days
# label = 1 if recency > 90 else 0

# Prepare static feature matrix
static_features = ['avg_order_value', 'avg_review_score',
                   'any_bad_review', 'on_time_delivery_ratio', 'lat_norm', 'lng_norm', 'avg_order_approval_hours',
                   'avg_delivery_days', 'on_time_delivery_ratio', 'avg_review_answer_days',
                   'preferred_payment_type', 'avg_payment_installments', 'avg_freight_value', 'avg_review_response_days',
                   'avg_customer_seller_distance']
# Note: We include preferred_payment_type which is categorical
# We will one-hot encode preferred_payment_type
static_df = pd.read_csv('/content/drive/MyDrive/Thesis/Datasets/customer_df_rf.csv', usecols=['customer_unique_id'] + static_features)
static_df.set_index('customer_unique_id', inplace=True)

# One-hot encode 'preferred_payment_type'
static_df = pd.get_dummies(static_df, columns=['preferred_payment_type'], prefix='pref_pay')

# Align static features with our sequence list order
X_static_all = []
for cust_id in customer_ids:
    # If customer not in static_df (should not happen if data is consistent), handle gracefully
    if cust_id in static_df.index:
        X_static_all.append(static_df.loc[cust_id].values.astype(float))
    else:
        X_static_all.append(np.zeros(static_df.shape[1]))  # placeholder if missing
X_static_all = np.stack(X_static_all)

# Optionally, scale static features (e.g., StandardScaler or MinMaxScaler)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_static_all = scaler.fit_transform(X_static_all)

# Determine class imbalance
import numpy as np
n_churn = np.sum(y_all == 1)
n_not_churn = np.sum(y_all == 0)
print(f"Churners: {n_churn}, Non-churners: {n_not_churn}")

# Compute class weights as inverse of frequency
total = len(y_all)
weight_for_churn = total / (2 * n_churn)
weight_for_not_churn = total / (2 * n_not_churn)
class_weight = {0: weight_for_not_churn, 1: weight_for_churn}
print("Class weight:", class_weight)

from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, BatchNormalization

def cnn_branch(img_input, hp=None):
    # 1st conv block
    filters1 = hp.Int('conv_filters1', 16, 64, step=16) if hp else 32
    x = Conv2D(filters1, 3, activation='relu', padding='same')(img_input)
    x = MaxPool2D()(x)                 # 32→16
    x = BatchNormalization()(x)

    # 2nd conv block  (always keep for 32×32)
    filters2 = hp.Int('conv_filters2', 32, 128, step=32) if hp else 64
    x = Conv2D(filters2, 3, activation='relu', padding='same')(x)
    x = MaxPool2D()(x)                 # 16→8
    x = BatchNormalization()(x)

    # Optional 3rd block controlled by hyper‑param
    if hp and hp.Boolean('third_conv'):
        filters3 = hp.Int('conv_filters3', 32, 128, step=32)
        x = Conv2D(filters3, 3, activation='relu', padding='same')(x)
        x = MaxPool2D()(x)             # 8→4
        x = BatchNormalization()(x)

    x = Flatten()(x)                   # 4×4×filters → manageable
    return x

from tensorflow.keras.layers import Input, LSTM, Bidirectional, Dense, Dropout, Masking, concatenate
from tensorflow.keras.models import Model


def build_hybrid_model(seq_input_shape, static_input_shape, img_input_shape,
                       num_lstm_layers=1, units_per_layer=[64],
                       bidirectional=False, dropout_rate=0.0, hp=None):
    """
    Build an LSTM model with given architecture parameters.
    seq_input_shape: tuple for sequence input (max_seq_len, num_features)
    static_input_shape: tuple for static input features (num_static_features,), or None if not used
    num_lstm_layers: how many LSTM layers to stack
    units_per_layer: list of LSTM units for each layer (length should == num_lstm_layers)
    bidirectional: whether to use Bidirectional LSTM for each layer
    dropout_rate: dropout to apply after each LSTM layer (except maybe last) and on final dense layer
    """
    # Define sequence input
    seq_input = Input(shape=seq_input_shape, name="sequence_input")
    x = Masking(mask_value=0.0, name="masking_layer")(seq_input)  # ignore padded zeros

    # Add LSTM layers
    for i in range(num_lstm_layers):
        units = units_per_layer[i]
        if bidirectional:
            x = Bidirectional(LSTM(units, return_sequences=(i < num_lstm_layers - 1),
                                    name=f"LSTM_layer_{i+1}",))(x)
        else:
            x = LSTM(units, return_sequences=(i < num_lstm_layers - 1), name=f"LSTM_layer_{i+1}")(x)
        # Apply dropout between layers if specified
        if dropout_rate > 0.0:
            x = Dropout(dropout_rate, name=f"dropout_layer_{i+1}")(x)

    # If static features are provided, define static input and concatenate
    if static_input_shape:
        static_input = Input(shape=static_input_shape, name="static_input")
        # (Optionally, you could add a Dense layer here to transform static input if needed)
        combined = concatenate([x, static_input], name="concatenate_layer")
    else:
        static_input = None
        combined = x

    # ➌ CNN branch (new)
    img_input = Input(shape=img_input_shape, name="image_input")
    x_img = cnn_branch(img_input, hp)

    # ➍ concatenate
    merged = concatenate([x, static_input, x_img])
    merged = Dense(64, activation='relu')(merged)
    if dropout_rate > 0.0:
        merged = Dropout(dropout_rate)(merged)
    output = Dense(1, activation='sigmoid')(merged)

    return Model(inputs=[seq_input, static_input, img_input], outputs=output)

# ==============================================================
# 0.  Numeric columns in the order‑sequence tensor
# ==============================================================
NUM_SEQ_FEATURES = 5               # first 5 columns are numeric

# ==============================================================
# 1.  Static‑feature cleaning *before* the split
#    • boolean→int → log‑transform → clip 99th pct
# ==============================================================
bool_cols = static_df.select_dtypes(bool).columns
static_df[bool_cols] = static_df[bool_cols].astype(int)

for col in ['avg_order_value','avg_order_approval_hours',
            'avg_customer_seller_distance','avg_freight_value']:
    static_df[col] = np.log1p(static_df[col])

for col in ['avg_delivery_days','avg_review_answer_days','avg_review_response_days']:
    p99 = static_df[col].quantile(0.99)
    static_df[col] = np.clip(static_df[col], None, p99)

# align & cast
X_static_all = static_df.reindex(customer_ids).astype('float32')

# ==============================================================
# 2.  Train / Val / Test split  (70 / 15 / 15), stratified
# ==============================================================
from sklearn.model_selection import train_test_split

X_seq_train, X_seq_temp, \
X_static_train_raw, X_static_temp_raw, \
X_img_train, X_img_temp, \
y_train, y_temp = train_test_split(
        X_seq_all, X_static_all, customer_images, y_all,
        test_size=0.30, stratify=y_all, random_state=42)

X_seq_val, X_seq_test, \
X_static_val_raw, X_static_test_raw, \
X_img_val, X_img_test, \
y_val, y_test = train_test_split(
        X_seq_temp, X_static_temp_raw, X_img_temp, y_temp,
        test_size=0.50, stratify=y_temp, random_state=42)

print("Shapes:", X_seq_train.shape, X_seq_val.shape, X_seq_test.shape)
print("Churn rates  train:", y_train.mean(),
      " val:", y_val.mean(), " test:", y_test.mean())

# ==============================================================
# 3.  Static  – fill NaNs with TRAIN mean  → RobustScaler
# ==============================================================
train_mean = X_static_train_raw.mean()
X_static_train_filled = X_static_train_raw.fillna(train_mean)
X_static_val_filled   = X_static_val_raw.fillna(train_mean)
X_static_test_filled  = X_static_test_raw.fillna(train_mean)

from sklearn.preprocessing import RobustScaler
stat_scaler = RobustScaler()
X_static_train = stat_scaler.fit_transform(X_static_train_filled)
X_static_val   = stat_scaler.transform(X_static_val_filled)
X_static_test  = stat_scaler.transform(X_static_test_filled)

# ==============================================================
# 4.  Sequence  – fit StandardScaler on *TRAIN* numeric slice
#     then transform val/test (no leakage)
# ==============================================================
from sklearn.preprocessing import StandardScaler
seq_scaler = StandardScaler()

flat_train = X_seq_train[:, :, :NUM_SEQ_FEATURES].reshape(-1, NUM_SEQ_FEATURES)
seq_scaler.fit(flat_train)

def scale_seq(arr):
    flat = arr[:, :, :NUM_SEQ_FEATURES].reshape(-1, NUM_SEQ_FEATURES)
    arr[:, :, :NUM_SEQ_FEATURES] = seq_scaler.transform(flat)\
                                             .reshape(arr.shape[0], -1, NUM_SEQ_FEATURES)
    return arr

X_seq_train = scale_seq(X_seq_train)
X_seq_val   = scale_seq(X_seq_val)
X_seq_test  = scale_seq(X_seq_test)

# ==============================================================
# 5.  Sanity checks
# ==============================================================
print("Static NaN:", np.isnan(X_static_train).any(),
      "Inf:", np.isinf(X_static_train).any())
print("Seq   NaN:", np.isnan(X_seq_train).any(axis=(1,2)).any())
print("Final shapes:", X_seq_train.shape, X_static_train.shape, X_img_train.shape)

import numpy as np, tensorflow as tf

# 1️⃣ Label shape / dtype
for name, y in [("train", y_train), ("val", y_val)]:
    print(f"{name}: shape {y.shape}, dtype {y.dtype}, pos_rate {y.mean():.3f}")

#  — labels must be float32 and shape (N,1) —
y_train = y_train.astype('float32').reshape(-1, 1)
y_val   = y_val.astype('float32').reshape(-1, 1)
y_test  = y_test.astype('float32').reshape(-1, 1)

# 2️⃣ NaN / Inf / crazy ranges in all inputs
def check(arr, tag):
    print(f"{tag:7s}  NaN? {np.isnan(arr).any()}  Inf? {np.isinf(arr).any()}  "
          f"range [{arr.min():.3g}, {arr.max():.3g}]")
check(X_seq_train,   "seq")
check(X_static_train,"static")
check(X_img_train,   "img")

# 3️⃣ Are all sequences zero after masking?
all_zero_pct = np.all(X_seq_train == 0, axis=(1,2)).mean()
print("100 %‑zero padded sequences:", all_zero_pct)

# convert to float32 and 2‑D
y_train = y_train.astype('float32').reshape(-1, 1)
y_val   = y_val.astype('float32').reshape(-1, 1)
y_test  = y_test.astype('float32').reshape(-1, 1)

import keras_tuner as kt
from tensorflow.keras import backend as K

def focal_loss(gamma=2., alpha=0.25):
    def _fl(y_true, y_pred):
        y_true = K.cast(y_true, K.floatx())
        # ❶ clip to prevent log(0)
        y_pred = K.clip(y_pred, K.epsilon(), 1. - K.epsilon())

        bce = K.binary_crossentropy(y_true, y_pred)
        pt  = K.exp(-bce)
        loss = alpha * K.pow(1. - pt, gamma) * bce
        # ❷ replace any remaining NaN/Inf with finite numbers
        loss = tf.where(tf.math.is_finite(loss), loss, tf.zeros_like(loss))
        return K.mean(loss)
    return _fl

# Hyperparameter tuning setup corrected
def build_model(hp):
    num_layers = hp.Int('num_lstm_layers', 1, 2)
    units = [hp.Choice(f'units_{i}', [32, 64]) for i in range(num_layers)]
    bidir = hp.Boolean('bidirectional')
    dropout_rate = hp.Float('dropout', 0.0, 0.3, step=0.1)


    model = build_hybrid_model(
        seq_input_shape=(max_seq_len, num_features),
        static_input_shape=(X_static_all.shape[1],),
        img_input_shape=X_img_train.shape[1:],
        num_lstm_layers=num_layers,
        units_per_layer=units,
        bidirectional=bidir,
        dropout_rate=dropout_rate,
        hp=hp
    )
# ── cosine‑decay LR schedule ──────────────────────────
    init_lr   = hp.Choice('lr_init', [3e-4, 1e-4, 5e-5])
    decay_steps = 30 * (len(X_seq_train) // 128)   # 30 epochs × steps/epoch
    lr_sched = tf.keras.optimizers.schedules.CosineDecay(
                   initial_learning_rate = init_lr,
                   decay_steps           = decay_steps,
                   alpha                 = 1e-5 / init_lr)   # final LR ≈1e‑5

    model.compile(
        optimizer=tf.keras.optimizers.Adam(lr_sched),
        loss=focal_loss(gamma=1.5, alpha=0.75),
        metrics=[tf.keras.metrics.AUC(name='AUC')]
    )
    return model


tuner = kt.Hyperband(
    build_model,
    objective='val_AUC',
    max_epochs=30,
    factor=2,
    directory='kt_tuning',
    project_name='olist_churn_hybrid'
)

# Run tuner WITHOUT class_weight
tuner.search(
    [X_seq_train, X_static_train, X_img_train], y_train,
    validation_data=([X_seq_val, X_static_val, X_img_val], y_val),
    epochs=20, batch_size=128,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(
            patience=3, restore_best_weights=True,
            monitor='val_AUC', mode='max'
        )
    ]
)

# Best hyperparameters from tuning
best_model = tuner.get_best_models(1)[0]

import os, datetime, pandas as pd
from sklearn.metrics import (roc_auc_score, average_precision_score)

# 1️⃣  model
best_model = tuner.get_best_models(1)[0]

SAVE_DIR  = "/content/drive/MyDrive/Thesis"
os.makedirs(SAVE_DIR, exist_ok=True)
MODEL_PATH = f"{SAVE_DIR}/hybrid_best_auc.keras"   # Keras v3 format

best_model.save(MODEL_PATH)       # no need for custom_objects with .keras
print("Model saved to:", MODEL_PATH)

# 2️⃣  metrics on test set
y_proba = best_model.predict([X_seq_test, X_static_test, X_img_test]).ravel()
metrics = {
    "timestamp" : datetime.datetime.utcnow().isoformat(timespec='seconds'),
    "roc_auc"   : roc_auc_score(y_test, y_proba),
    "pr_auc"    : average_precision_score(y_test, y_proba)
}
log_path = f"{SAVE_DIR}/metrics_log.csv"
pd.DataFrame([metrics]).to_csv(log_path, mode='a', index=False)
print("Metrics appended to:", log_path)

# 1️⃣  grab top‑3 models from the tuner
top3 = tuner.get_best_models(num_models=3)        # returns list of Models
for i, m in enumerate(top3, start=1):
    print(f"Model {i}  val_AUC = {m.evaluate([X_seq_val, X_static_val, X_img_val], y_val, verbose=0)[1]:.4f}")

# 2️⃣  helper that averages their predictions
def ensemble_predict(seq, stat, img, models=top3, bs=256):
    preds = [m.predict([seq, stat, img], batch_size=bs).ravel() for m in models]
    return np.mean(preds, axis=0)

# 3️⃣  evaluate on the test set
y_proba_ens = ensemble_predict(X_seq_test, X_static_test, X_img_test)
print("Ensemble ROC‑AUC:", roc_auc_score(y_test, y_proba_ens))
print("Ensemble PR‑AUC :", average_precision_score(y_test, y_proba_ens))

# 4️⃣  (optional) save ensemble probs for later analysis
np.save("/content/drive/MyDrive/Thesis/y_proba_ensemble.npy", y_proba_ens)

# 0️⃣  make sure labels are float32 column vectors
y_train = y_train.astype('float32').reshape(-1, 1)
y_val   = y_val.astype('float32').reshape(-1, 1)

# 1️⃣  compile with focal‑loss (α=0.75, γ=1.25)
best_model.compile(
    optimizer=tf.keras.optimizers.Adam(5e-5),
    loss=focal_loss(alpha=0.75, gamma=1.25),
    metrics=[tf.keras.metrics.AUC(name='AUC')]
)

# 2️⃣  callbacks
es   = tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True,
                                        monitor='val_AUC', mode='max')
ckpt = tf.keras.callbacks.ModelCheckpoint(
        "/content/drive/MyDrive/Thesis/fine_focal.keras",
        monitor='val_AUC', save_best_only=True, mode='max')

# 3️⃣  fine‑tune 3–5 epochs
best_model.fit([X_seq_train, X_static_train, X_img_train], y_train,
               validation_data=([X_seq_val, X_static_val, X_img_val], y_val),
               epochs=5, batch_size=128,
               callbacks=[es, ckpt], verbose=2)

from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, confusion_matrix, classification_report

# Get predicted probabilities for test set
y_proba = best_model.predict([X_seq_test, X_static_test, X_img_test]).ravel()
roc_auc = roc_auc_score(y_test, y_proba)
pr_auc = average_precision_score(y_test, y_proba)
print(f"Test ROC-AUC: {roc_auc:.4f}, Test PR-AUC: {pr_auc:.4f}")

# Find optimal threshold on validation (for example, maximize F1)
precision, recall, thresholds = precision_recall_curve(y_val, best_model.predict([X_seq_val, X_static_val, X_img_val]).ravel())
f1 = 2*(precision*recall)/(precision+recall+1e-8)
best_thresh = thresholds[np.argmax(f1)]
print(f"Optimal threshold: {best_thresh:.3f}")

# Apply threshold to test set
y_pred = (y_proba >= best_thresh).astype(int)
cm = confusion_matrix(y_test, y_pred)
print(classification_report(y_test, y_pred))

# Confusion matrix and classification report at this threshold
cm = confusion_matrix(y_test, y_pred)

print("Confusion Matrix:\n", cm)
print("Classification Report:\n", classification_report(y_test, y_pred, digits=4))

from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, confusion_matrix, classification_report
from sklearn.metrics import roc_curve
# Get predicted probabilities for test set
y_proba = best_model.predict([X_seq_test, X_static_test, X_img_test]).ravel()
roc_auc = roc_auc_score(y_test, y_proba)
pr_auc = average_precision_score(y_test, y_proba)
print(f"Test ROC-AUC: {roc_auc:.4f}, Test PR-AUC: {pr_auc:.4f}")
val_proba = best_model.predict([X_seq_val, X_static_val, X_img_val]).ravel()
test_proba = best_model.predict([X_seq_test, X_static_test, X_img_test]).ravel()


for delta in [-0.03, -0.05, -0.07]:
    best_thresh = 0.679 + delta
    y_pred = (test_proba >= best_thresh).astype(int)
    cm = confusion_matrix(y_test, y_pred)
    print(f"thr={best_thresh:.3f} | rec0={cm[0,0]/cm[0].sum():.2f}"
          f" prec0={cm[0,0]/cm[:,0].sum():.2f}"
          f" | rec1={cm[1,1]/cm[1].sum():.2f}"
          f" prec1={cm[1,1]/cm[:,1].sum():.2f}")

# Apply threshold to test set
y_pred = (y_proba >= best_thresh).astype(int)
cm = confusion_matrix(y_test, y_pred)
print(classification_report(y_test, y_pred))

# Confusion matrix and classification report at this threshold
cm = confusion_matrix(y_test, y_pred)

print("Confusion Matrix:\n", cm)
print("Classification Report:\n", classification_report(y_test, y_pred, digits=4))

chosen_thr = 0.649
y_pred = (y_proba >= chosen_thr).astype(int)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred, digits=4, zero_division=0))

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (roc_curve, precision_recall_curve,
                             confusion_matrix, classification_report, auc)

# ──────────────────────────────────────────
# 1)  choose the operating threshold
# ──────────────────────────────────────────
chosen_thr = 0.649           # <‑‑ your fixed threshold
print("Using threshold =", chosen_thr)

# probabilities were already computed:
# y_proba = best_model.predict([...]).ravel()

y_pred = (y_proba >= chosen_thr).astype(int)
cm     = confusion_matrix(y_test, y_pred)
print(classification_report(y_test, y_pred, digits=4, zero_division=0))

# ──────────────────────────────────────────
# 2)  ROC curve
# ──────────────────────────────────────────
fpr, tpr, _ = roc_curve(y_test, y_proba)
roc_auc_val = auc(fpr, tpr)

plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc_val:.2f})')
plt.plot([0,1], [0,1], 'k--', label='Random guess')
plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')
plt.title('ROC Curve – Test Set'); plt.legend(); plt.show()

# ──────────────────────────────────────────
# 3)  Precision‑Recall curve
# ──────────────────────────────────────────
prec, rec, _ = precision_recall_curve(y_test, y_proba)
pr_auc_val   = auc(rec, prec)

plt.figure(figsize=(6,5))
plt.plot(rec, prec, label=f'PR curve (AUC = {pr_auc_val:.2f})')
plt.xlabel('Recall'); plt.ylabel('Precision')
plt.title('Precision‑Recall Curve – Test Set'); plt.legend(); plt.show()

# ──────────────────────────────────────────
# 4)  Confusion‑matrix heat‑map at chosen_thr
# ──────────────────────────────────────────
plt.figure(figsize=(4,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Pred 0','Pred 1'],
            yticklabels=['True 0','True 1'])
plt.title(f'Confusion Matrix  (thr = {chosen_thr:.3f})')
plt.show()